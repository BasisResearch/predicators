"""Policy-guided planning for generalized policy generation (PG3).

PG3 requires known STRIPS operators. The command below uses oracle operators,
but it is also possible to use this approach with operators learned from
demonstrations.

Example command line:
    python predicators/main.py --approach pg3 --seed 0 \
        --env pddl_easy_delivery_procedural_tasks \
        --strips_learner oracle --num_train_tasks 10
"""
from __future__ import annotations

import abc
import functools
import logging
import time
from typing import Callable, Dict, FrozenSet, Iterator, List, Optional, \
    Sequence, Set, Tuple
from typing import Type as TypingType

import dill as pkl
from typing_extensions import TypeAlias

from predicators import utils
from predicators.approaches import ApproachFailure
from predicators.approaches.nsrt_learning_approach import NSRTLearningApproach
from predicators.planning import PlanningFailure, run_low_level_search
from predicators.settings import CFG
from predicators.structs import NSRT, Action, Box, Dataset, GroundAtom, \
    LDLRule, LiftedAtom, LiftedDecisionList, Object, ParameterizedOption, \
    Predicate, State, Task, Type, Variable, _GroundNSRT


class PG3Approach(NSRTLearningApproach):
    """Policy-guided planning for generalized policy generation (PG3)."""

    def __init__(self, initial_predicates: Set[Predicate],
                 initial_options: Set[ParameterizedOption], types: Set[Type],
                 action_space: Box, train_tasks: List[Task]) -> None:
        super().__init__(initial_predicates, initial_options, types,
                         action_space, train_tasks)
        self._current_ldl = LiftedDecisionList([])

    @classmethod
    def get_name(cls) -> str:
        return "pg3"

    def _predict_ground_nsrt(self, atoms: Set[GroundAtom],
                             objects: Set[Object],
                             goal: Set[GroundAtom]) -> _GroundNSRT:
        """Predicts next GroundNSRT to be deployed based on the PG3 generated
        policy."""
        ground_nsrt = utils.query_ldl(self._current_ldl, atoms, objects, goal)
        if ground_nsrt is None:
            raise ApproachFailure("PG3 policy was not applicable!")
        return ground_nsrt

    def _solve(self, task: Task, timeout: int) -> Callable[[State], Action]:
        """Searches for a low level policy that satisfies PG3's abstract
        policy."""
        skeleton = []
        atoms_sequence = []
        atoms = utils.abstract(task.init, self._initial_predicates)
        atoms_sequence.append(atoms)
        current_objects = set(task.init)
        start_time = time.perf_counter()

        while not task.goal.issubset(atoms):
            if (time.perf_counter() - start_time) >= timeout:
                raise ApproachFailure("Timeout exceeded")
            ground_nsrt = self._predict_ground_nsrt(atoms, current_objects,
                                                    task.goal)
            atoms = utils.apply_operator(ground_nsrt, atoms)
            skeleton.append(ground_nsrt)
            atoms_sequence.append(atoms)
        try:
            option_list, succeeded = run_low_level_search(
                task, self._option_model, skeleton, atoms_sequence, self._seed,
                timeout - (time.perf_counter() - start_time), CFG.horizon)
        except PlanningFailure as e:
            raise ApproachFailure(e.args[0], e.info)
        if not succeeded:
            raise ApproachFailure("Low-level search failed")
        policy = utils.option_plan_to_policy(option_list)
        return policy

    def _learn_ldl(self, online_learning_cycle: Optional[int]) -> None:
        """Learn a lifted decision list policy."""
        # Set up a search over LDL space.
        _S: TypeAlias = LiftedDecisionList
        # An "action" here is a search operator and an integer representing the
        # count of successors generated by that operator.
        _A: TypeAlias = Tuple[_PG3SearchOperator, int]

        # Create the PG3 search operators.
        search_operators = self._create_search_operators()

        # The heuristic is what distinguishes PG3 from baseline approaches.
        heuristic = self._create_heuristic()

        # Initialize the search.
        initial_state = self._get_policy_search_initial_ldl()

        def get_successors(ldl: _S) -> Iterator[Tuple[_A, _S, float]]:
            for op in search_operators:
                for i, child in enumerate(op.get_successors(ldl)):
                    yield (op, i), child, 1.0  # cost always 1

        if CFG.pg3_search_method == "gbfs":
            # Terminate only after max expansions.
            path, _ = utils.run_gbfs(
                initial_state=initial_state,
                check_goal=lambda _: False,
                get_successors=get_successors,
                heuristic=heuristic,
                max_expansions=CFG.pg3_gbfs_max_expansions,
                lazy_expansion=True)

        elif CFG.pg3_search_method == "hill_climbing":
            # Terminate when no improvement is found.
            path, _, _ = utils.run_hill_climbing(
                initial_state=initial_state,
                check_goal=lambda _: False,
                get_successors=get_successors,
                heuristic=heuristic,
                early_termination_heuristic_thresh=0,
                enforced_depth=CFG.pg3_hc_enforced_depth)

        else:
            raise NotImplementedError("Unrecognized pg3_search_method "
                                      f"{CFG.pg3_search_method}.")

        # Save the best seen policy.
        self._current_ldl = path[-1]
        logging.info(f"Keeping best policy:\n{self._current_ldl}")
        save_path = utils.get_approach_save_path_str()
        with open(f"{save_path}_{online_learning_cycle}.ldl", "wb") as f:
            pkl.dump(self._current_ldl, f)

    def learn_from_offline_dataset(self, dataset: Dataset) -> None:
        # First, learn NSRTs.
        self._learn_nsrts(dataset.trajectories, online_learning_cycle=None)
        # Now, learn the LDL policy.
        self._learn_ldl(online_learning_cycle=None)

    def load(self, online_learning_cycle: Optional[int]) -> None:
        # Load the NSRTs.
        super().load(online_learning_cycle)
        # Load the LDL policy.
        load_path = utils.get_approach_load_path_str()
        with open(f"{load_path}_{online_learning_cycle}.ldl", "rb") as f:
            self._current_ldl = pkl.load(f)

    def _create_search_operators(self) -> List[_PG3SearchOperator]:
        search_operator_classes = [
            _AddRulePG3SearchOperator,
            _AddConditionPG3SearchOperator,
        ]
        preds = self._get_current_predicates()
        nsrts = self._get_current_nsrts()
        return [_BottomUpPG3SearchOperator(preds, nsrts, self._create_heuristic())] + [cls(preds, nsrts) for cls in search_operator_classes]

    def _create_heuristic(self) -> _PG3Heuristic:
        preds = self._get_current_predicates()
        nsrts = self._get_current_nsrts()
        heuristic_name_to_cls: Dict[str, TypingType[_PG3Heuristic]] = {
            "policy_guided": _PolicyGuidedPG3Heuristic,
            "policy_evaluation": _PolicyEvaluationPG3Heuristic,
            "demo_plan_comparison": _DemoPlanComparisonPG3Heuristic,
        }
        cls = heuristic_name_to_cls[CFG.pg3_heuristic]
        return cls(preds, nsrts, self._train_tasks)

    @staticmethod
    def _get_policy_search_initial_ldl() -> LiftedDecisionList:
        # Initialize with an empty list by default, but subclasses may
        # override.
        return LiftedDecisionList([])


############################## Search Operators ###############################


class _PG3SearchOperator(abc.ABC):
    """Given an LDL policy, generate zero or more successor LDL policies."""

    def __init__(self, predicates: Set[Predicate], nsrts: Set[NSRT]) -> None:
        self._predicates = predicates
        self._nsrts = nsrts

    @abc.abstractmethod
    def get_successors(
            self, ldl: LiftedDecisionList) -> Iterator[LiftedDecisionList]:
        """Generate zero or more successor LDL policies."""
        raise NotImplementedError("Override me!")


class _AddRulePG3SearchOperator(_PG3SearchOperator):
    """An operator that adds new rules to an existing LDL policy."""

    def get_successors(
            self, ldl: LiftedDecisionList) -> Iterator[LiftedDecisionList]:
        for idx in range(len(ldl.rules) + 1):
            for rule in self._get_candidate_rules():
                new_rules = list(ldl.rules)
                new_rules.insert(idx, rule)
                yield LiftedDecisionList(new_rules)

    @functools.lru_cache(maxsize=None)
    def _get_candidate_rules(self) -> List[LDLRule]:
        return [self._nsrt_to_rule(nsrt) for nsrt in sorted(self._nsrts)]

    @staticmethod
    def _nsrt_to_rule(nsrt: NSRT) -> LDLRule:
        """Initialize an LDLRule from an NSRT."""
        return LDLRule(
            name=nsrt.name,
            parameters=list(nsrt.parameters),
            pos_state_preconditions=set(nsrt.preconditions),
            neg_state_preconditions=set(),
            goal_preconditions=set(),
            nsrt=nsrt,
        )


class _AddConditionPG3SearchOperator(_PG3SearchOperator):
    """An operator that adds new preconditions to existing LDL rules."""

    def get_successors(
            self, ldl: LiftedDecisionList) -> Iterator[LiftedDecisionList]:
        for rule_idx, rule in enumerate(ldl.rules):
            rule_vars = frozenset(rule.parameters)
            for condition in self._get_candidate_conditions(rule_vars):
                # Consider adding new condition to positive preconditions,
                # negative preconditions, or goal preconditions.
                for destination in ["pos", "neg", "goal"]:
                    new_pos = set(rule.pos_state_preconditions)
                    new_neg = set(rule.neg_state_preconditions)
                    new_goal = set(rule.goal_preconditions)
                    if destination == "pos":
                        dest_set = new_pos
                    elif destination == "neg":
                        dest_set = new_neg
                    else:
                        assert destination == "goal"
                        dest_set = new_goal
                    # If the condition already exists, skip.
                    if condition in dest_set:
                        continue
                    # Special case: if the condition already exists in the
                    # positive preconditions, don't add to the negative
                    # preconditions, and vice versa.
                    if destination in ("pos", "neg") and condition in \
                        new_pos | new_neg:
                        continue
                    dest_set.add(condition)
                    parameters = sorted({
                        v
                        for c in new_pos | new_neg | new_goal
                        for v in c.variables
                    } | set(rule.nsrt.parameters))
                    # Create the new rule.
                    new_rule = LDLRule(
                        name=rule.name,
                        parameters=parameters,
                        pos_state_preconditions=new_pos,
                        neg_state_preconditions=new_neg,
                        goal_preconditions=new_goal,
                        nsrt=rule.nsrt,
                    )
                    # Create the new LDL.
                    new_rules = list(ldl.rules)
                    new_rules[rule_idx] = new_rule
                    yield LiftedDecisionList(new_rules)

    @functools.lru_cache(maxsize=None)
    def _get_candidate_conditions(
            self, variables: FrozenSet[Variable]) -> List[LiftedAtom]:
        conditions = []
        for pred in sorted(self._predicates):
            # Create fresh variables for the predicate to complement the
            # variables that already exist in the rule.
            new_vars = utils.create_new_variables(pred.types, variables)
            for condition in utils.get_all_lifted_atoms_for_predicate(
                    pred, variables | frozenset(new_vars)):
                conditions.append(condition)
        return conditions


class _BottomUpPG3SearchOperator(_PG3SearchOperator):
    """An operator that adds new rules created from a bottom up method an existing LDL policy."""

    def __init__(self, predicates: Set[Predicate], nsrts: Set[NSRT], pg3_heuristic: _PlanComparisonPG3Heuristic) -> None:
        super().__init__(predicates, nsrts)
        self.heuristic = pg3_heuristic

    def get_successors(
            self, ldl: LiftedDecisionList) -> Iterator[LiftedDecisionList]:    
        kept_prob_idx = None
        shortest_plan_len = float("inf")
        kept_flattened_plan = None
        kept_problem_goals = None
        kept_actions = None
        kept_objects = None

        for task_idx in range(len(self.heuristic._abstract_train_tasks)):
            plan, actions = self.heuristic._get_plan_and_actions(ldl, task_idx)
            objects, _, goal = self.heuristic._abstract_train_tasks[task_idx]

            for t in range(len(plan)-1):
                state = plan[t]
                ground_nsrt = utils.query_ldl(ldl, state, objects, goal)
                #Found failed plan - policy has no action or action doesn't match with plan
                if ground_nsrt is None or utils.apply_operator(ground_nsrt, state) != plan[t+1]:
                    if len(plan) < shortest_plan_len:
                        kept_prob_idx = task_idx
                        kept_flattened_plan = plan
                        kept_problem_goals = goal
                        kept_actions = actions
                        kept_objects = objects
                        shortest_plan_len = len(plan)
        
        #Perfect policy TODO: CHECK THAT THIS ONLY RETURNS ONE POLICY WITH THE YIELD LATER
        print(f"KEPT PROB IDX{kept_prob_idx}")
        if kept_prob_idx is None:
            yield ldl

        action_profiles = self._get_action_profiles(kept_prob_idx, kept_flattened_plan, kept_actions, ldl)
        goal_indices = self._get_established_goal_indices(kept_problem_goals, action_profiles)

        triangle_table = TriangleTable(kept_flattened_plan[0].copy(), kept_problem_goals, action_profiles)

        #Find relevant indices of actions for each established goal ("directly-contributes" - recursive definition)
        failed_action_index = self._find_last_failed_action_index(action_profiles)
        closest_est_goal_index = shortestBFS(failed_action_index, triangle_table.reversed_adjacency_list, goal_indices.values())
        
        if closest_est_goal_index == None:
            yield ldl

        if failed_action_index == closest_est_goal_index:
            relevant_action_indices = []
        else:
            relevant_action_indices = [i for i in range(len(triangle_table.adjacency_list)) if distanceBFS(closest_est_goal_index, failed_action_index, triangle_table.adjacency_list)[i] is not None and i != failed_action_index]
        failed_action_profile = action_profiles[failed_action_index]
        est_goal = set(action_profiles[closest_est_goal_index].pos_effects) & kept_problem_goals
        relevant_action_profiles = [action_profiles[i] for i in relevant_action_indices]

        """
        print("ITERATION")
        print("states")
        for s in kept_flattened_plan:
            print(s)

        print("Failed action index", failed_action_index, "\n")
        print("Relevant action indices", relevant_action_indices, "\n")
        print("-----------")

        """
       
        #ldl = ldl.copy()
        #import ipdb; ipdb.set_trace()

        ldl = self._induce_policy_update(ldl, failed_action_profile, relevant_action_profiles, est_goal, kept_objects)


        yield ldl

    def _induce_policy_update(self, policy, failed_action_profile, relevant_action_profiles, est_goal, state_objects):
        """est_goal = set of goal literals satisfied in the established goal"""
        rule_goal_conds = est_goal

        rule_pos_preconds = set(failed_action_profile.pos_preconds) 
        rule_neg_preconds = set(failed_action_profile.neg_preconds) #Always include failed action preconds
        for goal_cond in est_goal:
            #not_goal_cond = goal_cond.predicate.get_negation()(goal_cond.objects) #TODO CHECK THIS
            rule_neg_preconds.add(goal_cond)

        ordered_action_profiles = [relevant_action_profile for relevant_action_profile in relevant_action_profiles]
        all_valid_preconds = set()
        #Forming the "context": a smaller state to work with
        for action_profile in ordered_action_profiles:
            all_valid_preconds = all_valid_preconds | (failed_action_profile.state & set(action_profile.pos_preconds))
        #Initialize important variables to be those in failed action and in goal condition
        important_variables = {var for goal_cond in est_goal for var in goal_cond.objects} | {var for var in failed_action_profile.variables} #Those in the goal literal and failed action

        #Initialize rule preconds to be those in valid preconds/context and all variables are important
        rule_pos_preconds = rule_pos_preconds | {precond for precond in all_valid_preconds if {var for var in precond.objects}.issubset(important_variables)}

        #Finding unique rule name
        existing_rule_names = {rule.name for rule in policy.rules}
        rule_name_suffix = 0
        rule_name = None
        while "TT-rule-" + str(rule_name_suffix) in existing_rule_names:
            rule_name_suffix += 1
        rule_name = "TT-rule-"+str(rule_name_suffix) 


        #Lifting rule
        rule_objects = {var for precond in rule_pos_preconds for var in precond.objects} | {var for precond in rule_neg_preconds for var in precond.objects}
        extra_rule_objects = rule_objects - {obj for obj in failed_action_profile.plan_action.objects} # Objects not in the nsrt for order purposes
        rule_object_to_var = {obj: obj.type("?" + obj.name) for obj in rule_objects}
        rule_variables = [obj.type("?" + obj.name) for obj in failed_action_profile.plan_action.objects] + [obj.type("?" + obj.name) for obj in extra_rule_objects] 
        rule_pos_preconds = {precond.lift(rule_object_to_var) for precond in rule_pos_preconds}

        rule_neg_preconds = {precond.lift(rule_object_to_var) for precond in rule_neg_preconds}
        rule_goal_conds = {precond.lift(rule_object_to_var) for precond in rule_goal_conds}
        rule_nsrt = self._lift_ground_nsrt(failed_action_profile.plan_action)

        rule = LDLRule(rule_name, rule_variables, rule_pos_preconds, rule_neg_preconds, rule_goal_conds, rule_nsrt)
        most_overfit_rule = rule

        temp_ldl = LiftedDecisionList([most_overfit_rule])
        temp_action = utils.query_ldl(temp_ldl, failed_action_profile.state, state_objects, est_goal)

        import ipdb;ipdb.set_trace()
        if temp_action == failed_action_profile.plan_action:
            new_policy = LiftedDecisionList([r for r in policy.rules])
            new_policy = self._insert_rule_in_last_valid(new_policy, failed_action_profile, state_objects, est_goal, rule)
            return new_policy

        for i in range(len(ordered_action_profiles)):
            #TODO HERE HERE HERE
            action_profile_variables = set(ordered_action_profiles[i].variables)
            if len(action_profile_variables - important_variables) == 0: #If no new variables, skip
                continue
            
            important_variables = important_variables | action_profile_variables
            rule_preconds = rule_preconds | {precond for precond in all_valid_preconds if {var for var in precond.variables}.issubset(important_variables)}
            rule = Rule(policy.env, rule_name, rule_preconds, rule_goal_conds, failed_action_profile.action)

            if i == len(ordered_action_profiles)-1:
                most_overfit_rule = rule
            try:
                temp_ldl = LiftedDecisionList([rule])
                temp_action = utils.query_ldl(temp_ldl, failed_action_profile.state, state_objects, est_goal)

                if rule.get_action(failed_action_profile.state) == failed_action_profile.action:
                    new_policy = LiftedDecisionList([r for r in policy.rules])
                    new_policy = self._insert_rule_in_last_valid(new_policy, failed_action_profile, state_objects, est_goal, rule)
                    return new_policy
            except:
                pass
        
        #If not found, return the most overfit
        new_policy = policy.copy()
        new_policy = self._insert_rule_in_last_valid(new_policy, failed_action_profile, most_overfit_rule)
        return new_policy 
    
    def _lift_ground_nsrt(self, ground_nsrt: _GroundNSRT) -> NSRT:
        objs = ground_nsrt.objects
        objs_to_vars = {obj: obj.type("?" + obj.name) for obj in ground_nsrt.objects}

        name = ground_nsrt.name
        parameters = [obj.type("?" + obj.name) for obj in ground_nsrt.objects]
        preconditions = {precond.lift(objs_to_vars) for precond in ground_nsrt.preconditions}
        add_effects = {effect.lift(objs_to_vars) for effect in ground_nsrt.add_effects}
        delete_effects = {effect.lift(objs_to_vars) for effect in ground_nsrt.delete_effects}
        ignore_effects = set()
        option = ground_nsrt.option
        option_vars = [obj.type("?" + obj.name) for obj in ground_nsrt.option_objs]
        _sampler = ground_nsrt._sampler

        return NSRT(name, parameters, preconditions, add_effects, delete_effects, ignore_effects, option, option_vars, _sampler)

    def _insert_rule_in_last_valid(self, policy, failed_action_profile, failed_action_objects, goal, rule):
        for idx, existing_rule in enumerate(policy.rules):
            # Does this rule apply?
            temp_ldl = LiftedDecisionList([existing_rule])
            temp_action = utils.query_ldl(temp_ldl, failed_action_profile.state, failed_action_objects, goal)

            if temp_action is not None:
                break
        else:
            idx = len(policy.rules)

        # Add in the rule
        policy.rules.insert(idx, rule)
        return policy

    def _find_last_failed_action_index(self, action_profiles):
        for i in range(len(action_profiles)-1, -1, -1):
            if action_profiles[i].is_failed:
                return i

    def _get_established_goal_indices(self, problem_goals, action_profiles):
        goal_indices = {}
        for i in range(len(action_profiles)-1, -1, -1):
            action_profile = action_profiles[i]
            current_state = action_profile.state
            unsatisfied_goal_conditions = problem_goals - current_state
            for unsatisfied_goal_condition in unsatisfied_goal_conditions:
                if unsatisfied_goal_condition not in goal_indices.keys():
                    goal_indices[unsatisfied_goal_condition] = i
        return goal_indices
    
    def _get_action_profiles(self, task_idx, plan_states, plan_actions, policy):
        action_profiles = []

        objects, _, goal = self.heuristic._abstract_train_tasks[task_idx]

        for i in range(len(plan_actions)):
            state = plan_states[i]
            policy_action = utils.query_ldl(policy, state, objects, goal)
            plan_action = plan_actions[i]
            plan_action_pos_preconds, plan_action_neg_preconds = self._get_ground_action_to_preconds(plan_action)
            plan_action_pos_effects = plan_action.add_effects
            plan_action_neg_effects = plan_action.delete_effects
            action_profile = TTActionProfile(plan_action, policy_action, state, plan_action_pos_preconds, plan_action_neg_preconds, plan_action_pos_effects, plan_action_neg_effects)
            action_profiles.append(action_profile)
        return action_profiles

    def _get_ground_action_to_preconds(self, action):
        """Returns pair of pos_preconds and neg_preconds for a ground action
        """
        pos_preconds = []
        neg_preconds = []
        for precond in action.preconditions:
            if str(precond.predicate).startswith("NOT-"):
                neg_preconds.append(precond)
            else:
                pos_preconds.append(precond)
        return pos_preconds, neg_preconds


class TTActionProfile():
    def __init__(self, plan_action, policy_action, state, pos_preconds, neg_preconds, pos_effects, neg_effects):
        self.plan_action = plan_action
        self.policy_action = policy_action
        if policy_action is None or plan_action != policy_action:
            self.is_failed = True
        else:
            self.is_failed = False
        self.state = state
        self.pos_preconds = pos_preconds
        self.neg_preconds = neg_preconds
        self.pos_effects = pos_effects
        self.neg_effects = neg_effects
        self.variables = self.plan_action.objects

    def __str__(self):
        return "\tAction:"+ str(self.plan_action) + "\nPos preconds"+ str(sorted(self.pos_preconds)) + "\nNeg preconds"+ str(sorted(self.neg_preconds)) + "\nPos Effects"+ str(sorted(self.pos_effects)) + "\nNeg Effects"+ str(sorted(self.neg_effects))

################################ Heuristics ###################################


class _PG3Heuristic(abc.ABC):
    """Given an LDL policy, produce a score, with lower better."""

    def __init__(
        self,
        predicates: Set[Predicate],
        nsrts: Set[NSRT],
        train_tasks: Sequence[Task],
    ) -> None:
        self._predicates = predicates
        self._nsrts = nsrts
        # Convert each train task into (object set, init atoms, goal).
        self._abstract_train_tasks = [(set(task.init),
                                       utils.abstract(task.init,
                                                      predicates), task.goal)
                                      for task in train_tasks]

    def __call__(self, ldl: LiftedDecisionList) -> float:
        """Compute the heuristic value for the given LDL policy."""
        score = 0.0
        for idx in range(len(self._abstract_train_tasks)):
            score += self._get_score_for_task(ldl, idx)
        logging.debug(f"Scoring:\n{ldl}\nScore: {score}")
        return score

    @abc.abstractmethod
    def _get_score_for_task(self, ldl: LiftedDecisionList,
                            task_idx: int) -> float:
        """Produce a score, with lower better."""
        raise NotImplementedError("Override me!")


class _PolicyEvaluationPG3Heuristic(_PG3Heuristic):
    """Score a policy based on the number of train tasks it solves at the
    abstract level."""

    def _get_score_for_task(self, ldl: LiftedDecisionList,
                            task_idx: int) -> float:
        objects, atoms, goal = self._abstract_train_tasks[task_idx]
        if self._ldl_solves_abstract_task(ldl, atoms, objects, goal):
            return 0.0
        return 1.0

    @staticmethod
    def _ldl_solves_abstract_task(ldl: LiftedDecisionList,
                                  atoms: Set[GroundAtom], objects: Set[Object],
                                  goal: Set[GroundAtom]) -> bool:
        for _ in range(CFG.horizon):
            if goal.issubset(atoms):
                return True
            ground_nsrt = utils.query_ldl(ldl, atoms, objects, goal)
            if ground_nsrt is None:
                return False
            atoms = utils.apply_operator(ground_nsrt, atoms)
        return goal.issubset(atoms)


class _PlanComparisonPG3Heuristic(_PG3Heuristic):
    """Score a policy based on agreement with certain plans.

    Which plans are used to compute agreement is defined by subclasses.
    """

    def __init__(
        self,
        predicates: Set[Predicate],
        nsrts: Set[NSRT],
        train_tasks: Sequence[Task],
    ) -> None:
        super().__init__(predicates, nsrts, train_tasks)
        # Ground the NSRTs once per task and save them.
        self._train_task_idx_to_ground_nsrts = {
            idx: [
                ground_nsrt for nsrt in nsrts
                for ground_nsrt in utils.all_ground_nsrts(nsrt, objects)
            ]
            for idx, (objects, _, _) in enumerate(self._abstract_train_tasks)
        }

    def _get_score_for_task(self, ldl: LiftedDecisionList,
                            task_idx: int) -> float:
        try:
            atom_plan = self._get_atom_plan_for_task(ldl, task_idx)
        except PlanningFailure:
            return CFG.horizon  # worst possible score
        # Note: we need the goal because it's an input to the LDL policy.
        objects, _, goal = self._abstract_train_tasks[task_idx]
        assert goal.issubset(atom_plan[-1])
        return self._count_missed_steps(ldl, atom_plan, objects, goal)

    @abc.abstractmethod
    def _get_atom_plan_for_task(self, ldl: LiftedDecisionList,
                                task_idx: int) -> Sequence[Set[GroundAtom]]:
        """Given a task, get the plan with which we will compare the policy.

        If no plan can be found, a PlanningFailure exception is raised.
        """
        raise NotImplementedError("Override me!")

    @staticmethod
    def _count_missed_steps(ldl: LiftedDecisionList,
                            atoms_seq: Sequence[Set[GroundAtom]],
                            objects: Set[Object],
                            goal: Set[GroundAtom]) -> float:
        missed_steps = 0.0
        for t in range(len(atoms_seq) - 1):
            ground_nsrt = utils.query_ldl(ldl, atoms_seq[t], objects, goal)
            if ground_nsrt is None:
                missed_steps += CFG.pg3_plan_compare_inapplicable_cost
            else:
                predicted_atoms = utils.apply_operator(ground_nsrt,
                                                       atoms_seq[t])
                if predicted_atoms != atoms_seq[t + 1]:
                    missed_steps += 1
        return missed_steps


class _DemoPlanComparisonPG3Heuristic(_PlanComparisonPG3Heuristic):
    """Score a policy based on agreement with demo plans.

    The demos are generated with a planner, once per train task.
    """

    def _get_atom_plan_for_task(self, ldl: LiftedDecisionList,
                                task_idx: int) -> Sequence[Set[GroundAtom]]:
        del ldl  # unused
        return self._get_demo_atom_plan_for_task(task_idx)

    @functools.lru_cache(maxsize=None)
    def _get_demo_atom_plan_for_task(
            self, task_idx: int) -> Sequence[Set[GroundAtom]]:
        # Run planning once per task and cache the result.

        objects, init, goal = self._abstract_train_tasks[task_idx]
        ground_nsrts = self._train_task_idx_to_ground_nsrts[task_idx]

        # Set up an A* search.
        _S: TypeAlias = FrozenSet[GroundAtom]
        _A: TypeAlias = _GroundNSRT

        def check_goal(atoms: _S) -> bool:
            return goal.issubset(atoms)

        def get_successors(atoms: _S) -> Iterator[Tuple[_A, _S, float]]:
            for op in utils.get_applicable_operators(ground_nsrts, atoms):
                next_atoms = utils.apply_operator(op, set(atoms))
                yield (op, frozenset(next_atoms), 1.0)

        heuristic = utils.create_task_planning_heuristic(
            heuristic_name=CFG.pg3_task_planning_heuristic,
            init_atoms=init,
            goal=goal,
            ground_ops=ground_nsrts,
            predicates=self._predicates,
            objects=objects,
        )

        planned_frozen_atoms_seq, _ = utils.run_astar(
            initial_state=frozenset(init),
            check_goal=check_goal,
            get_successors=get_successors,
            heuristic=heuristic)

        if not check_goal(planned_frozen_atoms_seq[-1]):
            raise PlanningFailure("Could not find plan for train task.")

        return [set(atoms) for atoms in planned_frozen_atoms_seq]


class _PolicyGuidedPG3Heuristic(_PlanComparisonPG3Heuristic):
    """Score a policy based on agreement with policy-guided plans."""

    def _get_atom_plan_for_task(self, ldl: LiftedDecisionList,
                                task_idx: int) -> Sequence[Set[GroundAtom]]:

        objects, init, goal = self._abstract_train_tasks[task_idx]
        ground_nsrts = self._train_task_idx_to_ground_nsrts[task_idx]

        # Set up a policy-guided A* search.
        _S: TypeAlias = FrozenSet[GroundAtom]
        _A: TypeAlias = _GroundNSRT

        def check_goal(atoms: _S) -> bool:
            return goal.issubset(atoms)

        def get_valid_actions(atoms: _S) -> Iterator[Tuple[_A, float]]:
            for op in utils.get_applicable_operators(ground_nsrts, atoms):
                yield (op, 1.0)

        def get_next_state(atoms: _S, ground_nsrt: _A) -> _S:
            return frozenset(utils.apply_operator(ground_nsrt, set(atoms)))

        heuristic = utils.create_task_planning_heuristic(
            heuristic_name=CFG.pg3_task_planning_heuristic,
            init_atoms=init,
            goal=goal,
            ground_ops=ground_nsrts,
            predicates=self._predicates,
            objects=objects,
        )

        def policy(atoms: _S) -> Optional[_A]:
            return utils.query_ldl(ldl, set(atoms), objects, goal)

        planned_frozen_atoms_seq, _ = utils.run_policy_guided_astar(
            initial_state=frozenset(init),
            check_goal=check_goal,
            get_valid_actions=get_valid_actions,
            get_next_state=get_next_state,
            heuristic=heuristic,
            policy=policy,
            num_rollout_steps=CFG.pg3_max_policy_guided_rollout,
            rollout_step_cost=0)

        if not check_goal(planned_frozen_atoms_seq[-1]):
            raise PlanningFailure("Could not find plan for train task.")

        return [set(atoms) for atoms in planned_frozen_atoms_seq]

    def _get_plan_and_actions(self, ldl: LiftedDecisionList,
                                task_idx: int) -> Sequence[Set[GroundAtom]]:

        objects, init, goal = self._abstract_train_tasks[task_idx]
        ground_nsrts = self._train_task_idx_to_ground_nsrts[task_idx]

        # Set up a policy-guided A* search.
        _S: TypeAlias = FrozenSet[GroundAtom]
        _A: TypeAlias = _GroundNSRT

        def check_goal(atoms: _S) -> bool:
            return goal.issubset(atoms)

        def get_valid_actions(atoms: _S) -> Iterator[Tuple[_A, float]]:
            for op in utils.get_applicable_operators(ground_nsrts, atoms):
                yield (op, 1.0)

        def get_next_state(atoms: _S, ground_nsrt: _A) -> _S:
            return frozenset(utils.apply_operator(ground_nsrt, set(atoms)))

        heuristic = utils.create_task_planning_heuristic(
            heuristic_name=CFG.pg3_task_planning_heuristic,
            init_atoms=init,
            goal=goal,
            ground_ops=ground_nsrts,
            predicates=self._predicates,
            objects=objects,
        )

        def policy(atoms: _S) -> Optional[_A]:
            return utils.query_ldl(ldl, set(atoms), objects, goal)

        planned_frozen_atoms_seq, action_seq = utils.run_policy_guided_astar(
            initial_state=frozenset(init),
            check_goal=check_goal,
            get_valid_actions=get_valid_actions,
            get_next_state=get_next_state,
            heuristic=heuristic,
            policy=policy,
            num_rollout_steps=CFG.pg3_max_policy_guided_rollout,
            rollout_step_cost=0)

        if not check_goal(planned_frozen_atoms_seq[-1]):
            raise PlanningFailure("Could not find plan for train task.")

        return [set(atoms) for atoms in planned_frozen_atoms_seq], action_seq

class TriangleTable():
    def __init__(self, init_state, problem_goals, profiled_plan):
        self.init_state = init_state
        self.problem_goals = problem_goals
        self.profiled_plan = profiled_plan

        self.n = len(profiled_plan)
        self.triangle_table = [[" " for _ in range(self.n + 1)] for _ in range(self.n + 1)]
        self.marked_init = [] #Marked clauses for operations (marked_init[i] = marked clauses for operation i)
        self.adjacency_list = [[] for _ in range(self.n)] #Note: this is a DAG directed backwards (e.g. for goal regression)
        self.reversed_adjacency_list = [[] for _ in range(self.n)] #Note: this is a DAG directed forwards (e.g. for goal finding)

        self.fill_column_zero()
        self.fill_delta_and_action()

    def fill_column_zero(self):
        current_state = self.init_state
        init_leftover = [None for _ in range(self.n+1)] #init_leftover[i] = leftovers after operation i-1 (so first entry is just the initial state)
        init_leftover[0] = current_state
        for i in range(len(self.profiled_plan)):
            current_marked_clauses = current_state & set(self.profiled_plan[i].pos_preconds) #Note, this won't take care of negative effects
            self.marked_init.append(current_marked_clauses)
            self.triangle_table[i][0] = InitSquare(current_state - current_marked_clauses, current_marked_clauses)
            #Only want to deletions, we don't care about additions
            for effect in self.profiled_plan[i].neg_effects:
                if effect in current_state:
                    current_state.remove(effect)
            init_leftover[i+1] = current_state

    def fill_delta_and_action(self):
        for i in range(len(self.profiled_plan)):
            action = self.profiled_plan[i].plan_action
            self.triangle_table[i][i+1] = ActionSquare(action) #Action Square
            curr_action_effects = set(self.profiled_plan[i].pos_effects)

            # (Automaticaly?) Filter out anti action effects
            """
            anti_action_effects = set()
            for effect in curr_action_effects:
                if effect.is_anti:
                    anti_action_effects.add(effect)
            curr_action_effects = curr_action_effects - anti_action_effects

            """
           
            for j in range(i+1, len(self.profiled_plan)):
                new_action_pos_preconds = set(self.profiled_plan[j].pos_preconds)
                marked_clauses = new_action_pos_preconds & curr_action_effects
                if len(marked_clauses) > 0:
                    self.adjacency_list[j].append((i, marked_clauses))
                    self.reversed_adjacency_list[i].append((j, marked_clauses))
                self.triangle_table[j][i+1] = DeltaSquare(curr_action_effects - marked_clauses, marked_clauses)
                for effect in self.profiled_plan[j].neg_effects:
                    if effect in curr_action_effects:
                        curr_action_effects.remove(effect) #Note, this won't take care of negative effects of the original action
            marked_clauses = self.problem_goals & curr_action_effects #Different from paper!
            self.triangle_table[self.n][i+1] = DeltaSquare(curr_action_effects - marked_clauses, marked_clauses)

    """
     def display(self):
        cell_text = [[str(obj) for obj in self.triangle_table[i]] for i in range(len(self.triangle_table))]
        table = plt.table(cellText = cell_text, loc = "center", cellLoc = "center")
        table.auto_set_font_size(False)
        table.set_fontsize(5)
        table.scale(1, 2)
        plt.axis('off')
        plt.show()

    """
   
class TriangleTableSquare():
    def __init__(self):
        self.prob_idx = None
        self.prob_goals = None

class DeltaSquare(TriangleTableSquare):
    def __init__(self, unmarked_clauses, marked_clauses):
        self.unmarked_clauses = unmarked_clauses 
        self.marked_clauses = marked_clauses

    def __str__(self):
        entire_str = "Marked:\n"
        for marked_clause in self.marked_clauses:
            entire_str += str(marked_clause) + "\n"
        
        entire_str += "Unmarked\n"
        for unmarked_clause in self.unmarked_clauses:
            entire_str += str(unmarked_clause) + "\n"
        return entire_str

class InitSquare(TriangleTableSquare):
    def __init__(self, unmarked_clauses, marked_clauses):
        self.leftovers = unmarked_clauses 
        self.marked_clauses = marked_clauses

    def __str__(self):
        entire_str = "Marked:\n"
        for marked_clause in self.marked_clauses:
            entire_str += str(marked_clause) + "\n"
        return entire_str

class ActionSquare(TriangleTableSquare):
    def __init__(self, action):
        self.action =  action

    def __str__(self):
        return str(self.action)

def distanceBFS(start, block, adjacency_list):
    """Returns list mapping index to shortest distance
    Should be used from an established goal (start) to find directly-contributing actions
    block = failing action (and should not search from that node)
    """
    distances = [None for _ in range(len(adjacency_list))]
    queue = [(start, 0)]
    while len(queue) != 0:
        node, distance = queue.pop(0)
        if distances[node] is not None: #If already visited
            continue
        distances[node] = distance
        for neighbor, marked_clauses in adjacency_list[node]:
            if neighbor > block:
                queue.append((neighbor, distance+1))
    return distances

def shortestBFS(start, adjacency_list, goals):
    distances = [None for _ in range(len(adjacency_list))]
    queue = [(start, 0)]
    while len(queue) != 0:
        node, distance = queue.pop(0)
        if distances[node] is not None: #If already visited
            continue
        if node in goals:
            return node
        distances[node] = distance
        for neighbor, marked_clauses in adjacency_list[node]:
            queue.append((neighbor, distance+1))
    return None