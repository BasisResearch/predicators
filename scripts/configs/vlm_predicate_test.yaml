# Use VLM to propose predicates.
---
APPROACHES:
  learn_all:
    NAME: "oracle"
    FLAGS: []
ENVS:
  stick_button:
    # NAME: "stick_button"
    # NAME: "stick_button_move"
    NAME: "cover_place_hard"
    # NAME: "cover_multistep_options"
    # NAME: "doors"
    # NAME: "painting"
ARGS: 
  # []
  - "debug"
  # - "rgb_observation" # stick_button doesn't support rendering
  # - "make_failure_videos"
  # - "make_test_videos"
  # - "make_segmented_demo_videos"
  # - "make_demo_videos" 
  # - "use_gui"
FLAGS:
  num_train_tasks: 20
  num_test_tasks: 50
  # option_learner: direct_bc # So it doesn' include all oracle options in init
                            # when including all options, should also include
                            # all predicates.
  option_learner: no_learning # include all options by flagging no learning
  # included_options: "HandPressButton"
  # stick_button_button_position: "only_bottom"
  # stick_button_button_position: "only_top"
  # bilevel_plan_without_sim: True
  excluded_predicates: "all"
  sesame_max_samples_per_step: 10
  min_perc_data_for_nsrt: 1
  segmenter: "contacts"
  neural_gaus_regressor_max_itr: 50000
  vlm_model_name: "gpt-3.5-turbo-0125"
  timeout: 300
  pybullet_camera_width: 1674 
  pybullet_camera_height: 900
  video_fps: 20
  sesame_max_skeletons_optimized: 200

START_SEED: 2
NUM_SEEDS: 1
...
