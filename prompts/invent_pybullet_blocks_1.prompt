You are an expert AI researcher tasked with inventing task-specific state abstraction predicates for effective and efficient robotic planning.

I will describe the API you should use for writing predicates and the environment the robot is in.
# API for Defining Predicates
Class definitions for `Predicate` and `State` are:
```python
class RawState:
    """
    A class representing the raw visual state of the world

    Attributes:
    -----------
    labeled_image : PIL.Image.Image
        An observation of the state of the world annotated with an unique label 
        for each object.
    obj_mask_dict : Dict[str, Mask]
        A dictionary mapping object names to their corresponding segmentation 
        mask.

    Methods:
    --------
    crop_to_objects(self, objects: Collection[Object],
                    left_margin: int = 5, lower_margin: int=10, 
                    right_margin: int=10, top_margin: int=5) -> Image:
        Crops the labeled image to only focus on the objects in the input.
    get(self, obj: Object, feature_name: str) -> Any:
        This method looks up an object feature by name. It returns the value of 
        the feature.
    get_objects(self, object_type: Type) -> List[Object]:
        This method returns objects of the given type in the order of 
        __iter__().
    """
    def crop_to_objects(self, objects: Collection[Object],
                        left_margin: int = 5,
                        lower_margin: int=10, 
                        right_margin: int=10, 
                        top_margin: int=5) -> Image:
        """
        Crop the labeled image observation of the state to only include the 
        specified objects.

        The cropping is done by utilizing the masks of the objects, with optional 
        margins around the objects.

        Parameters:
        -----------
        objects : Collection[Object]
            The objects to include in the cropped image.
        left_margin : int, optional
            The left margin to include in the cropped image (default is 5).
        lower_margin : int, optional
            The lower margin to include in the cropped image (default is 10).
        right_margin : int, optional
            The right margin to include in the cropped image (default is 10).
        top_margin : int, optional
            The top margin to include in the cropped image (default is 5).

        Returns:
        --------
        Image
            The cropped image.
        
        Example 1:
        --------
        >>> An example for classifying Covers
        >>> def _Covers_NSP_holds(state: State, objects: Sequence[Object]
        >>>                         ) -> bool:
        >>>     '''
        >>>     Determine if the block is covering (directly on top of) the target 
        >>>     region.
        >>>     '''
        >>>     block, target = objects
        >>>
        >>>     # Necessary but not sufficient condition for covering: no part of the 
        >>>     # target region is outside the block.
        >>>     if state.get(target, "bbox_left") < state.get(block, "bbox_left") or\
                   state.get(target, "bbox_right") > state.get(block, "bbox_right"):
        >>>         return False
        >>>     ...
        >>>     return evaluate_simple_assertion(...)
        Example 2: 
        ----------
        >>> # An example for predicate OnTable
        >>> def _OnTable_NSP_holds(state: RawState, objects:Sequence[Object]) ->\
        >>>         bool:
        >>>     '''Determine if the block in objects is directly resting on the table's 
        >>>     surface in the scene image.
        >>>     '''
        >>>     block, = objects
        >>>     block_name = block.id_name
        >>>     
        >>>     # Crop the scene image to the smallest bounding box that include both
        >>>     # objects.
        >>>     # We know there is only one table in this environment.
        >>>     table = state.get_objects(_table_type)[0]
        >>>     table_name = table.id_name
        >>>     attention_image = state.crop_to_objects([block, table])

        >>>     return evaluate_simple_assertion(
        >>>         f"{block_name} is directly resting on {table_name}'s surface.",
        >>>         attention_image)
        """

    def get(self, obj: Object, feature_name: str) -> Any:
        """
        Look up an object feature by name.

        Parameters:
        -----------
        obj : Object
            The object whose feature value is to be retrieved.
        feature_name : str
            The name of the feature to be retrieved.

        Returns:
        --------
        Any
            The value of the specified feature for the given object.

        Raises:
        -------
        ValueError
            If the specified feature name is not found in the object's type feature names.
        
        Example 1:
        ---------
        >>> _robot_type = Type("robot", ["x", "y", "tilt", "wrist", "fingers"])
        >>> def _WristBent_holds(state: State, objects: Sequence[Object]
        >>>                     ) -> bool:
        >>>     robot, = objects
        >>>     return state.get(robot, "wrist") >= 0.5
        >>> _WristBent = NSPredicate("WristBent", [_robot_type], _WristBent_holds)

        Example 2:
        ----------
        >>> # An example for predicate On
        >>> def _On_NSP_holds(state: RawState, objects: Sequence[Object])\
        >>>     -> bool:
        >>>     '''
        >>>     Determine if the first block in objects is directly on top of the second 
        >>>     block in the scene image, by using simple heuristics and image processing 
        >>>     techniques.
        >>>     '''
        >>>     block1, block2 = objects
        >>>
        >>>     if state.get(block1, "bbox_lower") < state.get(block2, "bbox_lower")or\
        >>>      state.get(block1, "bbox_left") > state.get(block2, "bbox_right") or\
        >>>      state.get(block1, "bbox_right") < state.get(block2, "bbox_left") or\
        >>>      state.get(block1, "bbox_upper") < state.get(block2, "bbox_upper") or\
        >>>      state.get(block1, "pose_z") < state.get(block2, "pose_z"):
        >>>       return False
        >>>     ...
        """

    def get_objects(self, object_type: Type) -> List[Object]:
        """
        Return objects of the given type in the state

        Parameters:
        -----------
        object_type : Type
            The type of the objects to be retrieved.

        Returns:
        --------
        List[Object]
            A list of objects of the specified type, in the order they are 
            iterated over in the state.

        Examples:
        ---------
        >>> def _robot_hand_above_cup(state: State, cup: Object) -> bool:
        >>>     ...
        >>>
        >>> def _HandNotAboveCup_holds(state: State,
        >>>                            objects: Sequence[Object]) -> bool:
        >>>     for cup in state.get_objects(_cup_type):
        >>>         if _robot_hand_above_cup(state, cup):
        >>>             return False
        >>>     return True
        >>> _HandNotAboveCup = NSPredicate("HandNotAboveCup", [], 
        >>>                              _HandNotAboveCup_holds)
        """

class NSPredicate:
    """
    A class representing a predicate, a classifier that characterizes properties 
    of states in the context of AI task planning.
    A predicate is a function that takes a state and a sequence of objects as 
    input, and returns a boolean value indicating whether a certain  property 
    holds for those objects in that state.

    Parameters:
    -----------
    name : str
        The name of the predicate.

    types : Sequence[Type]
        The types of the objects that the predicate applies to. This sequence 
        length should match the number of objects passed to the classifier. Each
        type corresponds one-to-one with an object in the sequence. 

    _classifier : Callable[[State, Sequence[Object]], bool]
        The classifier function for the predicate. It takes a state and a
        sequence of objects as input, and returns a boolean value. The sequence
        of objects should correspond one-to-one with the 'types' attribute. The 
        classifier returns True if the predicate holds for those objects in that 
        state, and False otherwise.
    """    

def evaluate_simple_assertion(assertion: str, image: Image) -> bool:
    """
    Evaluate a simple assertion about an image by querying a vision language 
    model (VLM).

    This function is a helper that can be used in writing _classifier functions 
    for NSPredicates. It takes a simple assertion as a string and an image as 
    input, and returns a boolean value indicating whether the assertion holds 
    true for the image according to the VLM.

    Note that VLM has limited visual understanding so the assertion should be 
    clear, unambiguous, and relatively simple, and the image should have been 
    cropped to only the relavant objects.
    Moreover, don't write heuristics or rules that are not always true.

    Parameters:
    -----------
    assertion : str
        The assertion to be evaluated. This should be a clear, unambiguous, and 
        relatively simple statement about the image.

    image : Image
        The image for which the assertion is to be evaluated.

    Returns:
    --------
    bool
        True if the VLM determines that the assertion holds true for the image, 
        False otherwise.
    """
```


# The Environment
The environment includes the following object-type variables:
```python
_block_type = Type("block", [])
_robot_type = Type("robot", ["pose_x", "pose_y", "pose_z", "fingers"])
_table_type = Type("table", [])
```


The existing set of predicates (with example definitions) are:
{'On(?x:block, ?y:block)', 'OnTable(?x:block)'}

Predicate OnTable(?x) is defined by:
```python
def _OnTable_NSP_holds(state: RawState, objects:Sequence[Object]) ->\
        bool:
    '''Determine if the block in objects is directly resting on the table's 
    surface in the scene image.
    This method uses simple heuristics and image processing techniques to 
    determine the spatial relationship between the block and the table. 
    It first identifies the table in the scene, then crops the scene image 
    to the smallest bounding box that includes both the block and the table, 
    and finally evaluates a simple assertion about their relative positions.

    Parameters:
    -----------
    state : RawState
        The current state of the world, represented as an image.
    objects : Sequence[Object]
        A sequence containing a single block whose relationship with the 
        table is to be determined.

    Returns:
    --------
    bool
        True if the block is directly resting on the table's surface, False 
        otherwise.
    '''
    block, = objects
    block_name = block.id_name


    # Crop the scene image to the smallest bounding box that include both
    # objects.
    # We know there is only one table in this environment.
    table = state.get_objects(_table_type)[0]
    table_name = table.id_name
    attention_image = state.crop_to_objects([block, table])

    return evaluate_simple_assertion(
        f"{block_name} is directly resting on {table_name}'s surface.",
        attention_image)
_OnTable_NSP = NSPredicate("OnTable", [_block_type],
                                    _OnTable_NSP_holds)
```

Predicate On(?x, ?y) is defined by:
```python
def _On_NSP_holds(state: RawState, objects: Sequence[Object]) -> bool:
    '''
    Determine if the first block in objects is directly on top of the second 
    block in the scene image.

    This method uses simple heuristics and image processing techniques to 
    determine the spatial relationship between the two blocks. It first 
    checks if the blocks are the same or if they are far away from each 
    other. If neither condition is met, it crops the scene image to the 
    smallest bounding box that includes both blocks and evaluates a simple 
    assertion about their relative positions.

    Parameters:
    -----------
    state : RawState
        The current state of the world, represented as an image.
    objects : Sequence[Object]
        A sequence of two blocks whose relationship is to be determined. The 
        first block is the one that is potentially on top.

    Returns:
    --------
    bool
        True if the first block is directly on top of the second block with 
        no blocks in between, False otherwise.
    '''

    block1, block2 = objects
    block1_name, block2_name = block1.id_name, block2.id_name

    # Heuristics: we know a block can't be on top of it
    if block1_name == block2_name:
        return False

    # repeat the above
    if state.get(block1, "bbox_lower") < state.get(block2, "bbox_lower")or\
       state.get(block1, "bbox_left") > state.get(block2, "bbox_right") or\
       state.get(block1, "bbox_right") < state.get(block2, "bbox_left") or\
       state.get(block1, "bbox_upper") < state.get(block2, "bbox_upper") or\
       state.get(block1, "pose_z") < state.get(block2, "pose_z"):
        return False

    # Crop the scene image to the smallest bounding box that include both
    # objects.
    attention_image = state.crop_to_objects([block1, block2])

    return evaluate_simple_assertion(
        f"{block1_name} is directly on top of {block2_name} with no blocks"+
         " in between.", attention_image)
_On_NSP = NSPredicate("On", [_block_type, _block_type],
                                    _On_NSP_holds)
```


The existing abstract action operators utilizing the predicates are:
Operator-Unstack:
    Parameters: [?block:block, ?otherblock:block, ?robot:robot]
    Preconditions: [On(?block:block, ?otherblock:block)]
    Add Effects: []
    Delete Effects: [On(?block:block, ?otherblock:block)]
    Ignore Effects: []
    Option Spec: Pick(?robot:robot, ?block:block)
Operator-PutOnTable:
    Parameters: [?block:block, ?robot:robot]
    Preconditions: []
    Add Effects: [OnTable(?block:block)]
    Delete Effects: []
    Ignore Effects: []
    Option Spec: PutOnTable(?robot:robot)
Operator-PickFromTable:
    Parameters: [?block:block, ?robot:robot]
    Preconditions: [OnTable(?block:block)]
    Add Effects: []
    Delete Effects: [OnTable(?block:block)]
    Ignore Effects: []
    Option Spec: Pick(?robot:robot, ?block:block)
Operator-Stack:
    Parameters: [?block:block, ?otherblock:block, ?robot:robot]
    Preconditions: []
    Add Effects: [On(?block:block, ?otherblock:block)]
    Delete Effects: []
    Ignore Effects: []
    Option Spec: Stack(?robot:robot, ?otherblock:block)

The robot tried to excute its options and got the following results:
Ground option Stack(robot1:robot, block4:block) was applied on 68 states and *successfully* executed on 1/68 states (ground truth positive states).
  Out of the 1 GT positive states, with the current predicates and operators, 1/1 states *satisfy* at least one of its operators' precondition (true positives):
  In state_32 with additional info:
  {'block3:block': {'pose_x': 1.3, 'pose_y': 1.0, 'pose_z': 0.2, 'bbox_left': 557.0, 'bbox_right': 595.0, 'bbox_upper': 434.0, 'bbox_lower': 397.0},
   'block4:block': {'pose_x': 1.3, 'pose_y': 1.0, 'pose_z': 0.3, 'bbox_left': 560.0, 'bbox_right': 598.0, 'bbox_upper': 463.0, 'bbox_lower': 424.0},
   'block5:block': {'pose_x': 1.4, 'pose_y': 0.8, 'pose_z': 0.7, 'bbox_left': 457.0, 'bbox_right': 497.0, 'bbox_upper': 761.0, 'bbox_lower': 721.0},
   'robot1:robot': {'pose_x': 1.4, 'pose_y': 0.8, 'pose_z': 0.7, 'fingers': 0.0, 'bbox_left': 267.0, 'bbox_right': 628.0, 'bbox_upper': 899.0, 'bbox_lower': 463.0},
   'table2:table': {'bbox_left': 26.0, 'bbox_right': 774.0, 'bbox_upper': 462.0, 'bbox_lower': 63.0}}

Ground option Stack(robot1:robot, block4:block) was applied on 68 states and *failed* to executed on 67/68 states (ground truth negative states).
  Out of the 67 GT negative states, with the current predicates and operators, 67/67 states *satisfy* at least one of its operators' precondition (false positives), to list 3:
  In state_0 with additional info:
  {'block3:block': {'pose_x': 1.4, 'pose_y': 0.9, 'pose_z': 0.2, 'bbox_left': 510.0, 'bbox_right': 547.0, 'bbox_upper': 427.0, 'bbox_lower': 387.0},
   'block4:block': {'pose_x': 1.4, 'pose_y': 0.5, 'pose_z': 0.2, 'bbox_left': 189.0, 'bbox_right': 228.0, 'bbox_upper': 429.0, 'bbox_lower': 390.0},
   'block5:block': {'pose_x': 1.4, 'pose_y': 0.5, 'pose_z': 0.3, 'bbox_left': 185.0, 'bbox_right': 225.0, 'bbox_upper': 457.0, 'bbox_lower': 417.0},
   'block6:block': {'pose_x': 1.4, 'pose_y': 0.6, 'pose_z': 0.2, 'bbox_left': 297.0, 'bbox_right': 331.0, 'bbox_upper': 426.0, 'bbox_lower': 385.0},
   'robot1:robot': {'pose_x': 1.4, 'pose_y': 0.8, 'pose_z': 0.7, 'fingers': 1.0, 'bbox_left': 267.0, 'bbox_right': 542.0, 'bbox_upper': 899.0, 'bbox_lower': 463.0},
   'table2:table': {'bbox_left': 26.0, 'bbox_right': 774.0, 'bbox_upper': 462.0, 'bbox_lower': 63.0}}

  In state_1 with additional info:
  {'block3:block': {'pose_x': 1.4, 'pose_y': 0.5, 'pose_z': 0.2, 'bbox_left': 193.0, 'bbox_right': 233.0, 'bbox_upper': 425.0, 'bbox_lower': 385.0},
   'block4:block': {'pose_x': 1.4, 'pose_y': 0.5, 'pose_z': 0.3, 'bbox_left': 190.0, 'bbox_right': 230.0, 'bbox_upper': 452.0, 'bbox_lower': 413.0},
   'block5:block': {'pose_x': 1.4, 'pose_y': 0.5, 'pose_z': 0.3, 'bbox_left': 186.0, 'bbox_right': 227.0, 'bbox_upper': 482.0, 'bbox_lower': 442.0},
   'block6:block': {'pose_x': 1.4, 'pose_y': 0.5, 'pose_z': 0.4, 'bbox_left': 182.0, 'bbox_right': 224.0, 'bbox_upper': 512.0, 'bbox_lower': 471.0},
   'robot1:robot': {'pose_x': 1.4, 'pose_y': 0.8, 'pose_z': 0.7, 'fingers': 1.0, 'bbox_left': 267.0, 'bbox_right': 542.0, 'bbox_upper': 899.0, 'bbox_lower': 463.0},
   'table2:table': {'bbox_left': 26.0, 'bbox_right': 774.0, 'bbox_upper': 462.0, 'bbox_lower': 63.0}}

  In state_2 with additional info:
  {'block3:block': {'pose_x': 1.3, 'pose_y': 0.9, 'pose_z': 0.2, 'bbox_left': 507.0, 'bbox_right': 542.0, 'bbox_upper': 434.0, 'bbox_lower': 394.0},
   'block4:block': {'pose_x': 1.3, 'pose_y': 0.5, 'pose_z': 0.2, 'bbox_left': 212.0, 'bbox_right': 250.0, 'bbox_upper': 432.0, 'bbox_lower': 393.0},
   'block5:block': {'pose_x': 1.3, 'pose_y': 0.5, 'pose_z': 0.3, 'bbox_left': 209.0, 'bbox_right': 248.0, 'bbox_upper': 460.0, 'bbox_lower': 420.0},
   'robot1:robot': {'pose_x': 1.4, 'pose_y': 0.8, 'pose_z': 0.7, 'fingers': 1.0, 'bbox_left': 267.0, 'bbox_right': 542.0, 'bbox_upper': 899.0, 'bbox_lower': 463.0},
   'table2:table': {'bbox_left': 26.0, 'bbox_right': 774.0, 'bbox_upper': 462.0, 'bbox_lower': 63.0}}

Ground option Stack(robot1:robot, block3:block) was applied on 86 states and *successfully* executed on 1/86 states (ground truth positive states).
  Out of the 1 GT positive states, with the current predicates and operators, 1/1 states *satisfy* at least one of its operators' precondition (true positives):
  In state_43 with additional info:
  {'block3:block': {'pose_x': 1.4, 'pose_y': 0.6, 'pose_z': 0.2, 'bbox_left': 255.0, 'bbox_right': 291.0, 'bbox_upper': 424.0, 'bbox_lower': 384.0},
   'block4:block': {'pose_x': 1.4, 'pose_y': 0.8, 'pose_z': 0.7, 'bbox_left': 426.0, 'bbox_right': 465.0, 'bbox_upper': 761.0, 'bbox_lower': 721.0},
   'block5:block': {'pose_x': 1.4, 'pose_y': 1.0, 'pose_z': 0.2, 'bbox_left': 556.0, 'bbox_right': 595.0, 'bbox_upper': 424.0, 'bbox_lower': 383.0},
   'robot1:robot': {'pose_x': 1.4, 'pose_y': 0.8, 'pose_z': 0.7, 'fingers': 0.0, 'bbox_left': 267.0, 'bbox_right': 609.0, 'bbox_upper': 899.0, 'bbox_lower': 463.0},
   'table2:table': {'bbox_left': 26.0, 'bbox_right': 774.0, 'bbox_upper': 462.0, 'bbox_lower': 63.0}}

Ground option Stack(robot1:robot, block3:block) was applied on 86 states and *failed* to executed on 85/86 states (ground truth negative states).
  Out of the 85 GT negative states, with the current predicates and operators, 85/85 states *satisfy* at least one of its operators' precondition (false positives), to list 3:
  In state_0 with additional info:
  {'block3:block': {'pose_x': 1.4, 'pose_y': 0.9, 'pose_z': 0.2, 'bbox_left': 510.0, 'bbox_right': 547.0, 'bbox_upper': 427.0, 'bbox_lower': 387.0},
   'block4:block': {'pose_x': 1.4, 'pose_y': 0.5, 'pose_z': 0.2, 'bbox_left': 189.0, 'bbox_right': 228.0, 'bbox_upper': 429.0, 'bbox_lower': 390.0},
   'block5:block': {'pose_x': 1.4, 'pose_y': 0.5, 'pose_z': 0.3, 'bbox_left': 185.0, 'bbox_right': 225.0, 'bbox_upper': 457.0, 'bbox_lower': 417.0},
   'block6:block': {'pose_x': 1.4, 'pose_y': 0.6, 'pose_z': 0.2, 'bbox_left': 297.0, 'bbox_right': 331.0, 'bbox_upper': 426.0, 'bbox_lower': 385.0},
   'robot1:robot': {'pose_x': 1.4, 'pose_y': 0.8, 'pose_z': 0.7, 'fingers': 1.0, 'bbox_left': 267.0, 'bbox_right': 542.0, 'bbox_upper': 899.0, 'bbox_lower': 463.0},
   'table2:table': {'bbox_left': 26.0, 'bbox_right': 774.0, 'bbox_upper': 462.0, 'bbox_lower': 63.0}}

  In state_1 with additional info:
  {'block3:block': {'pose_x': 1.4, 'pose_y': 0.5, 'pose_z': 0.2, 'bbox_left': 193.0, 'bbox_right': 233.0, 'bbox_upper': 425.0, 'bbox_lower': 385.0},
   'block4:block': {'pose_x': 1.4, 'pose_y': 0.5, 'pose_z': 0.3, 'bbox_left': 190.0, 'bbox_right': 230.0, 'bbox_upper': 452.0, 'bbox_lower': 413.0},
   'block5:block': {'pose_x': 1.4, 'pose_y': 0.5, 'pose_z': 0.3, 'bbox_left': 186.0, 'bbox_right': 227.0, 'bbox_upper': 482.0, 'bbox_lower': 442.0},
   'block6:block': {'pose_x': 1.4, 'pose_y': 0.5, 'pose_z': 0.4, 'bbox_left': 182.0, 'bbox_right': 224.0, 'bbox_upper': 512.0, 'bbox_lower': 471.0},
   'robot1:robot': {'pose_x': 1.4, 'pose_y': 0.8, 'pose_z': 0.7, 'fingers': 1.0, 'bbox_left': 267.0, 'bbox_right': 542.0, 'bbox_upper': 899.0, 'bbox_lower': 463.0},
   'table2:table': {'bbox_left': 26.0, 'bbox_right': 774.0, 'bbox_upper': 462.0, 'bbox_lower': 63.0}}

  In state_2 with additional info:
  {'block3:block': {'pose_x': 1.3, 'pose_y': 0.9, 'pose_z': 0.2, 'bbox_left': 507.0, 'bbox_right': 542.0, 'bbox_upper': 434.0, 'bbox_lower': 394.0},
   'block4:block': {'pose_x': 1.3, 'pose_y': 0.5, 'pose_z': 0.2, 'bbox_left': 212.0, 'bbox_right': 250.0, 'bbox_upper': 432.0, 'bbox_lower': 393.0},
   'block5:block': {'pose_x': 1.3, 'pose_y': 0.5, 'pose_z': 0.3, 'bbox_left': 209.0, 'bbox_right': 248.0, 'bbox_upper': 460.0, 'bbox_lower': 420.0},
   'robot1:robot': {'pose_x': 1.4, 'pose_y': 0.8, 'pose_z': 0.7, 'fingers': 1.0, 'bbox_left': 267.0, 'bbox_right': 542.0, 'bbox_upper': 899.0, 'bbox_lower': 463.0},
   'table2:table': {'bbox_left': 26.0, 'bbox_right': 774.0, 'bbox_upper': 462.0, 'bbox_lower': 63.0}}

Ground option Stack(robot1:robot, block6:block) was applied on 30 states and *failed* to executed on 30/30 states (ground truth negative states).
  Out of the 30 GT negative states, with the current predicates and operators, 30/30 states *satisfy* at least one of its operators' precondition (false positives), to list 3:
  In state_0 with additional info:
  {'block3:block': {'pose_x': 1.4, 'pose_y': 0.9, 'pose_z': 0.2, 'bbox_left': 510.0, 'bbox_right': 547.0, 'bbox_upper': 427.0, 'bbox_lower': 387.0},
   'block4:block': {'pose_x': 1.4, 'pose_y': 0.5, 'pose_z': 0.2, 'bbox_left': 189.0, 'bbox_right': 228.0, 'bbox_upper': 429.0, 'bbox_lower': 390.0},
   'block5:block': {'pose_x': 1.4, 'pose_y': 0.5, 'pose_z': 0.3, 'bbox_left': 185.0, 'bbox_right': 225.0, 'bbox_upper': 457.0, 'bbox_lower': 417.0},
   'block6:block': {'pose_x': 1.4, 'pose_y': 0.6, 'pose_z': 0.2, 'bbox_left': 297.0, 'bbox_right': 331.0, 'bbox_upper': 426.0, 'bbox_lower': 385.0},
   'robot1:robot': {'pose_x': 1.4, 'pose_y': 0.8, 'pose_z': 0.7, 'fingers': 1.0, 'bbox_left': 267.0, 'bbox_right': 542.0, 'bbox_upper': 899.0, 'bbox_lower': 463.0},
   'table2:table': {'bbox_left': 26.0, 'bbox_right': 774.0, 'bbox_upper': 462.0, 'bbox_lower': 63.0}}

  In state_1 with additional info:
  {'block3:block': {'pose_x': 1.4, 'pose_y': 0.5, 'pose_z': 0.2, 'bbox_left': 193.0, 'bbox_right': 233.0, 'bbox_upper': 425.0, 'bbox_lower': 385.0},
   'block4:block': {'pose_x': 1.4, 'pose_y': 0.5, 'pose_z': 0.3, 'bbox_left': 190.0, 'bbox_right': 230.0, 'bbox_upper': 452.0, 'bbox_lower': 413.0},
   'block5:block': {'pose_x': 1.4, 'pose_y': 0.5, 'pose_z': 0.3, 'bbox_left': 186.0, 'bbox_right': 227.0, 'bbox_upper': 482.0, 'bbox_lower': 442.0},
   'block6:block': {'pose_x': 1.4, 'pose_y': 0.5, 'pose_z': 0.4, 'bbox_left': 182.0, 'bbox_right': 224.0, 'bbox_upper': 512.0, 'bbox_lower': 471.0},
   'robot1:robot': {'pose_x': 1.4, 'pose_y': 0.8, 'pose_z': 0.7, 'fingers': 1.0, 'bbox_left': 267.0, 'bbox_right': 542.0, 'bbox_upper': 899.0, 'bbox_lower': 463.0},
   'table2:table': {'bbox_left': 26.0, 'bbox_right': 774.0, 'bbox_upper': 462.0, 'bbox_lower': 63.0}}

  In state_3 with additional info:
  {'block3:block': {'pose_x': 1.4, 'pose_y': 0.9, 'pose_z': 0.2, 'bbox_left': 491.0, 'bbox_right': 526.0, 'bbox_upper': 426.0, 'bbox_lower': 389.0},
   'block4:block': {'pose_x': 1.4, 'pose_y': 0.9, 'pose_z': 0.3, 'bbox_left': 493.0, 'bbox_right': 529.0, 'bbox_upper': 456.0, 'bbox_lower': 416.0},
   'block5:block': {'pose_x': 1.4, 'pose_y': 0.9, 'pose_z': 0.3, 'bbox_left': 494.0, 'bbox_right': 531.0, 'bbox_upper': 485.0, 'bbox_lower': 445.0},
   'block6:block': {'pose_x': 1.3, 'pose_y': 0.4, 'pose_z': 0.2, 'bbox_left': 172.0, 'bbox_right': 212.0, 'bbox_upper': 435.0, 'bbox_lower': 395.0},
   'robot1:robot': {'pose_x': 1.4, 'pose_y': 0.8, 'pose_z': 0.7, 'fingers': 1.0, 'bbox_left': 267.0, 'bbox_right': 542.0, 'bbox_upper': 899.0, 'bbox_lower': 463.0},
   'table2:table': {'bbox_left': 26.0, 'bbox_right': 774.0, 'bbox_upper': 462.0, 'bbox_lower': 63.0}}


Your objective is to invent all necessary predicates such that when they are added to the action operators' precondition, the states where the ground option fail to execute (GT negative states) no longer satisfy any of its ground operators' precondition, while the states where the ground option successfully executed (GT positive state) still satisfy at least one of its operators' precondition. 
That is, in future planning, the operators with the invented predicates should help to maximize the number of true positive and negative states while minimizing the number of false positive and negative states.

In your answer, carefully examine and analyze the differences between the GT positive and negative states that could affect the success or failure of executing the ground option.
Then propose predicates in paragraphs as follows. For each predicate:
- Briefly explain which aspect in the negative states for the ground option does it identify while it's satisfied by the GT positive states.
- Define predicate in a python block as follows:
```python
def classifier(state: State, objects: Sequence[Object]) -> bool:
    # Implement the boolean classifier function here
    ...
    
name: str = ... # Define the predicate name here
param_types: Sequence[Type] = ... # A list of object-type variables for the predicate, using the ones defined in the environment<predicate_name> = NSPredicate(name, param_types, classifier)
```

Ensure that:
- Take the predefined predicate definition as reference;
- Use only object-type variables defined in the environment when defining  `param_types`.
- The proposed predicates are semantically distinct from each other and from the existing predicates.
- Don't use any constant that is not defined.
- Make use of the helper method `evaluate_simple_assertion` in classifers that can't perfectly defined with just bounding boxes.
- Make use of the helper methods from the State class such as `crop_to_objects`, `get` and `get_objects` or other helper function that you have defined;
- The object feature names used in classifier definitions are present in the states and object type instantiations;
- Strictly follow the type hints in the predicate definition template.
You don't need to write out the new operators.